import json
import logging
import os
import random
import time 
from datetime import datetime

import pandas as pd
import yfinance as yf
from confluent_kafka import Producer
from dotenv import load_dotenv
from typing import Optional

#Load Env Variables
load_dotenv()

#Configure Logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s [%(levelname)s] %(message)s'
)

logger = logging.getLogger(__name__)

#Kafka Variables
KAFKA_BOOTSTRAP_SERVER=os.getenv('KAFKA_BOOTSTRAP_SERVER')
KAFKA_TOPIC_REALTIME=os.getenv('KAFKA_TOPIC_REALTIME')

# Define stocks with initial prices
STOCKS = {
    'AAPL': 180.0, # Apple
    'MSFT': 350.0, # Microsoft
    'GOOGL': 130.0, # Alphabet (Google)
    'AMZN': 130.0, # Amazon
    'META': 300.0, # Meta (Facebook)
    'TSLA': 200.0, # Tesla
    'NVDA': 400.0, # NVIDIA
    'INTC': 35.0, # Intel
}

class StreamDataCollector:
    def __init__(self, bootstrap_servers=KAFKA_BOOTSTRAP_SERVER, topic=KAFKA_TOPIC_REALTIME, interval=30):

        self.logger = logger
        self.topic = topic
        self.interval = interval

        self.current_stocks = STOCKS.copy()

        self.producer_conf = {
            "bootstrap.servers": bootstrap_servers,
            "client.id": "continuous-stock-data-producer"
        }

        try:
            self.producer = Producer(self.producer_conf)
            self.logger.info(f"Producer initialized. Sending to: {bootstrap_servers}, Topic: {topic}")
        except Exception as e:
            self.logger.error(f"Failed to create Kafka Producer: {e}")
            raise

    def delivery_report(self, err, msg):
        if err is not None:
            self.logger.error(f"Delivery failed for message: {msg}")
        else:
            self.logger.info(f"Message delivered successfully to topic {msg.topic} [{msg.partition()}]")

    def generate_stock_data(self, symbol):
        current_price = self.current_stocks[symbol]

        market_factor = random.uniform(-0.005, 0.005) # market wide movement
        stock_factor = random.uniform(-0.005, 0.005) # stock specific movement

        change_pct = market_factor + stock_factor

        if random.random() < 0.05:
            stock_factor += random.uniform(-0.02, 0.02)

        new_price = round(current_price * (1+change_pct), 2)

        self.current_stocks[symbol] = new_price

        #calculate percent change
        price_change = round(new_price - current_price, 2)
        percent_change = round((price_change / current_price) * 100, 2)

        volume = random.randint(1000, 100000)

        stock_data = {
            "symbol": symbol,
            "price": new_price,
            "change": price_change,
            "percent_change": percent_change,
            "volume": volume,
            "timestamp": datetime.now().isoformat()
        }

        return stock_data

    def produce_stock_data(self):
        """Main function to continuously produce stock data."""
        self.logger.info(f"Starting continuous stock data production")

        try:
            while True:
                # Track successful and failed symbols
                successful_symbols = 0
                failed_symbols = 0

                # Fetch and produce data for each stock
                for symbol in self.current_stocks.keys():
                    try:
                        # Generate stock data
                        stock_data = self.generate_stock_data(symbol)

                        if stock_data:
                            # Convert to JSON string
                            message = json.dumps(stock_data)

                            # Publish to Kafka topic
                            self.producer.produce(
                                self.topic,
                                key=symbol,
                                value=message,
                                callback=self.delivery_report
                            )

                            #Trigger message delivery
                            self.producer.poll(0)

                            successful_symbols += 1

                        else:
                            failed_symbols += 1

                    except Exception as e:
                        self.logger.error(f"Error processing {symbol}: {e}")
                        failed_symbols += 1

                # Log summary of data generation
                self.logger.info(f"Data Generation Summary: Successful: {successful_symbols}, Failed: {failed_symbols}")

                # Wait before next iteration
                self.logger.info(f"Waiting {self.interval} seconds before next data generation...")
                time.sleep(self.interval)

        except KeyboardInterrupt:
            self.logger.info("Producer stopped by user")
        except Exception as e:
            self.logger.error(f"Unexpected error: {e}")
        finally:
            self.logger.info("Producer shutting down")
            self.producer.flush()

def main():
    """Main function to run the Kafka Producer"""
    try:
        logger.info(f"Starting Streaming Stock Data Collector")

        producer = StreamDataCollector(
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVER,
            topic=KAFKA_TOPIC_REALTIME,
            interval=5 # Generate data every 30 seconds
        )

        # start producing stock data
        producer.produce_stock_data()

    except Exception as e:
        logger.error(f"Fatal error: {e}")

if __name__ == "__main__":
    main()